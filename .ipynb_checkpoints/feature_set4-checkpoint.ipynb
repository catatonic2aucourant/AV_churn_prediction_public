{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import category_encoders as en\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, make_union\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "\n",
    "#from OneHotEncoderDf import OneHotEncoderDf\n",
    "#from LabelEncoderDf import LabelEncoderDf\n",
    "#from FeatureSubsetTransformer import FeatureSubsetTransformer\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from skopt import BayesSearchCV, gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt import dump\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "#import xgboost as xgb\n",
    "\n",
    "#import category_encoders as en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_proba_corr(self, X):\n",
    "    preds = self.predict_proba(X)[:,1]\n",
    "    d0 = 0.5\n",
    "    d1 = 1 - d0\n",
    "    r0 = np.mean(preds)\n",
    "    r1 = 1 - r0\n",
    "    gamma_0 = r0/d0\n",
    "    gamma_1 = r1/d1\n",
    "    return gamma_1*preds/(gamma_1*preds + gamma_0*(1 - preds))\n",
    "\n",
    "def eval_top(y, preds):\n",
    "    y = y.values\n",
    "    n = int(len(preds) * 0.2)\n",
    "    indices = np.argsort(preds)[::-1][:n]\n",
    "    #print(len(indices))\n",
    "    return sum(y[indices])/sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\", low_memory=False)\n",
    "test = pd.read_csv(\"../data/test.csv\", low_memory=False)\n",
    "#data_df = train.append(test)\n",
    "# base_feats = [f for f in data_df.columns if f not in [\"UCIC_ID\", \"Responders\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = list(train.select_dtypes(['object']).columns.values)\n",
    "for col in cat_cols:\n",
    "    lb = LabelEncoder()\n",
    "    lb.fit(pd.concat([train[col], test[col]]).astype(str).fillna('-1'))\n",
    "    train[col] = lb.transform(train[col].astype(str).fillna('-1'))\n",
    "    test[col] = lb.transform(test[col].astype(str).fillna('-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_feats = [f for f in train.columns if f not in ['UCIC_ID','Responders'] + list(cat_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[num_feats] = train[num_feats].fillna(0)\n",
    "test[num_feats] = test[num_feats].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train = data_df.iloc[:train.shape[0]]\n",
    "#test = data_df.iloc[train.shape[0]:]\n",
    "X = train[num_feats + cat_cols]\n",
    "y = train['Responders']\n",
    "X_test = test[num_feats + cat_cols]\n",
    "\n",
    "cvlist = list(StratifiedKFold(n_splits=5, random_state=5).split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets encode all categoricals with likelihood\n",
    "#Writing cv method completely in pandas\n",
    "def cv_feat_pd1col(data_df, eval_col, target_col, cvlist, func, thresh=3):\n",
    "    cv_vals = np.ones(len(data_df)) * np.nan #Initialize\n",
    "    for tr_index, val_index in cvlist:\n",
    "        tr_func_dict = data_df.loc[tr_index].groupby(eval_col)[target_col].apply(lambda x: \n",
    "                                                    func(x) if len(x) >= thresh  else np.nan).to_dict()\n",
    "        #print(tr_func_dict)\n",
    "        cv_vals[val_index] = data_df.loc[val_index, eval_col].apply(lambda row: tr_func_dict[row]\n",
    "                                                                     if row in tr_func_dict\n",
    "                                                                     else func(data_df.loc[tr_index, target_col]))\n",
    "    return cv_vals\n",
    "\n",
    "def cv_feat_pdcols(data_df, eval_cols, target_col, cvlist, func, thresh=3):\n",
    "    cv_vals = np.ones(len(data_df)) * np.nan #Initialize\n",
    "    X = data_df.copy()\n",
    "    X['eval_records'] = data_df[eval_cols].to_records(index=False)\n",
    "    return cv_feat_pd1col(X, 'eval_records', target_col, cvlist, func, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  HNW_CATEGORY\n",
      "Processing  OCCUP_ALL_NEW\n",
      "Processing  city\n",
      "Processing  FINAL_WORTH_prev1\n",
      "Processing  EFT_SELF_TRANSFER_PrevQ1\n",
      "Processing  Charges_cnt_PrevQ1_N\n",
      "Processing  gender_bin\n",
      "Processing  RBI_Class_Audit\n",
      "Processing  zip\n",
      "Processing  brn_code\n",
      "Processing  age\n"
     ]
    }
   ],
   "source": [
    "#Get likehood features\n",
    "ltrain = len(train)\n",
    "ltest  = len(test)\n",
    "test['Responders'] = np.nan\n",
    "all_data = pd.concat([train, test]).reset_index(drop=True)\n",
    "cvtraintest = [[np.arange(ltrain), np.arange(ltrain, ltrain + ltest)]]\n",
    "\n",
    "\n",
    "for col in ['HNW_CATEGORY', 'OCCUP_ALL_NEW', 'city', 'FINAL_WORTH_prev1', 'EFT_SELF_TRANSFER_PrevQ1',\n",
    "           'Charges_cnt_PrevQ1_N', 'gender_bin', 'RBI_Class_Audit', 'zip', 'brn_code', 'age']:\n",
    "    print(\"Processing \", col)\n",
    "    train[col+'_likelihood'] = cv_feat_pd1col(train, col, 'Responders', cvlist, np.mean, 50)\n",
    "    test[col+'_likelihood'] = cv_feat_pd1col(all_data, col, 'Responders', cvtraintest, np.mean, 50)[ltrain:]\n",
    "    \n",
    "del all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[\"EOP_prev1_cat\"] = pd.qcut(train[\"EOP_prev1\"], q = 50, labels = False, duplicates = \"drop\")\n",
    "test[\"EOP_prev1_cat\"] = pd.qcut(test[\"EOP_prev1\"], q = 50, labels = False, duplicates = \"drop\")\n",
    "\n",
    "train[\"age_cat\"] = [1 if x <= 18 else 0 for x in train[\"age\"]]\n",
    "test[\"age_cat\"] = [1 if x <= 18 else 0 for x in test[\"age\"]]\n",
    "\n",
    "cat_cols.append(\"EOP_prev1_cat\")\n",
    "cat_cols.append(\"age_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mean of of drops; std in drops\n",
    "train.loc[:,'EOP_mean_drop'] = train[['EOP_prev1', \"EOP_prev2\", 'EOP_prev3',\n",
    "                               'EOP_prev4', \"EOP_prev5\", 'EOP_prev6',]].apply(\n",
    "                                                            lambda r: np.mean(r), axis=1)\n",
    "train.loc[:,'EOP_std_drop'] = train[['EOP_prev1', \"EOP_prev2\", 'EOP_prev3',\n",
    "                               'EOP_prev4', \"EOP_prev5\", 'EOP_prev6',]].apply(\n",
    "                                                            lambda r: np.std(r), axis=1)\n",
    "\n",
    "test.loc[:,'EOP_mean_drop'] = test[['EOP_prev1', \"EOP_prev2\", 'EOP_prev3',\n",
    "                               'EOP_prev4', \"EOP_prev5\", 'EOP_prev6',]].apply(\n",
    "                                                            lambda r: np.mean(r), axis=1)\n",
    "test.loc[:,'EOP_std_drop'] = test[['EOP_prev1', \"EOP_prev2\", 'EOP_prev3',\n",
    "                               'EOP_prev4', \"EOP_prev5\", 'EOP_prev6',]].apply(\n",
    "                                                            lambda r: np.std(r), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mean of of drops; std in drops\n",
    "train.loc[:,'CR_mean_drop'] = train[['CR_AMB_Drop_Build_1', 'CR_AMB_Drop_Build_2', \n",
    "                               'CR_AMB_Drop_Build_3', 'CR_AMB_Drop_Build_4',\n",
    "                              'CR_AMB_Drop_Build_5']].apply(lambda r: np.mean(r), axis=1)\n",
    "train.loc[:,'CR_std_drop'] = train[['CR_AMB_Drop_Build_1', 'CR_AMB_Drop_Build_2', \n",
    "                               'CR_AMB_Drop_Build_3', 'CR_AMB_Drop_Build_4',\n",
    "                              'CR_AMB_Drop_Build_5']].apply(lambda r: np.std(r), axis=1)\n",
    "\n",
    "test.loc[:,'CR_mean_drop'] = test[['CR_AMB_Drop_Build_1', 'CR_AMB_Drop_Build_2', \n",
    "                               'CR_AMB_Drop_Build_3', 'CR_AMB_Drop_Build_4',\n",
    "                              'CR_AMB_Drop_Build_5']].apply(lambda r: np.mean(r), axis=1)\n",
    "test.loc[:,'CR_std_drop'] = test[['CR_AMB_Drop_Build_1', 'CR_AMB_Drop_Build_2', \n",
    "                               'CR_AMB_Drop_Build_3', 'CR_AMB_Drop_Build_4',\n",
    "                              'CR_AMB_Drop_Build_5']].apply(lambda r: np.std(r), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sum of drops\n",
    "train.loc[:,'CR_sum_drop'] = train[['CR_AMB_Drop_Build_1', 'CR_AMB_Drop_Build_2', \n",
    "                               'CR_AMB_Drop_Build_3', 'CR_AMB_Drop_Build_4',\n",
    "                              'CR_AMB_Drop_Build_5']].apply(lambda r: np.sum(r), axis=1)\n",
    "\n",
    "test.loc[:,'CR_sum_drop'] = test[['CR_AMB_Drop_Build_1', 'CR_AMB_Drop_Build_2', \n",
    "                               'CR_AMB_Drop_Build_3', 'CR_AMB_Drop_Build_4',\n",
    "                              'CR_AMB_Drop_Build_5']].apply(lambda r: np.sum(r), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ratio average balance to EOB and CR of last month\n",
    "train.loc[:,'EOB_rat1'] = train['EOP_prev1']/(1 + train['I_AQB_PrevQ1'])\n",
    "train.loc[:,'CR_rat1'] = train['CR_AMB_Drop_Build_1']/(1 + train['I_AQB_PrevQ1'])\n",
    "\n",
    "test.loc[:,'EOB_rat1'] = test['EOP_prev1']/(1 + test['I_AQB_PrevQ1'])\n",
    "test.loc[:,'CR_rat1'] = test['CR_AMB_Drop_Build_1']/(1 + test['I_AQB_PrevQ1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.loc[:,'EOB_rat2'] = train['EOP_prev1']/(1 + train['I_AQB_PrevQ2'].clip(0, 10**9))\n",
    "train.loc[:,'CR_rat2'] = train['CR_AMB_Drop_Build_1']/(1 + train['I_AQB_PrevQ2'].clip(0, 10**9))\n",
    "\n",
    "test.loc[:,'EOB_rat2'] = test['EOP_prev1']/(1 + test['I_AQB_PrevQ2'].clip(0, 10**9))\n",
    "test.loc[:,'CR_rat2'] = test['CR_AMB_Drop_Build_1']/(1 + test['I_AQB_PrevQ2'].clip(0, 10**9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ratio of balances\n",
    "train.loc[:,'bal_rats'] = train['I_AQB_PrevQ2']/(1 + train['I_AQB_PrevQ1'])\n",
    "test.loc[:,'bal_rats'] = test['I_AQB_PrevQ2']/(1 + test['I_AQB_PrevQ1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#log values\n",
    "train.loc[:,'EOP_prev1log'] = np.log1p(train['EOP_prev1'].clip(0, 10**9))\n",
    "test.loc[:,'EOP_prev1log'] = np.log1p(test['EOP_prev1'].clip(0, 10**9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encode branch code and city by average balnces of their customers\n",
    "train.loc[:,'brn_bal'] = train.groupby('brn_code')['I_AQB_PrevQ1'].transform(np.mean)\n",
    "test.loc[:,'brn_bal'] = test.groupby('brn_code')['I_AQB_PrevQ1'].transform(np.mean)\n",
    "\n",
    "train.loc[:,'zip_bal'] = train.groupby('zip')['I_AQB_PrevQ1'].transform(np.mean)\n",
    "test.loc[:,'zip_bal'] = test.groupby('zip')['I_AQB_PrevQ1'].transform(np.mean)\n",
    "\n",
    "train.loc[:,'brn_churn'] = train.groupby('brn_code')['CR_rat1'].transform(np.mean)\n",
    "test.loc[:,'brn_churn'] = test.groupby('brn_code')['CR_rat1'].transform(np.mean)\n",
    "\n",
    "train.loc[:,'zip_churn'] = train.groupby('zip')['CR_rat1'].transform(np.mean)\n",
    "test.loc[:,'zip_churn'] = test.groupby('zip')['CR_rat1'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decline features\n",
    "train.loc[:,'bal_decline6'] = (train['BAL_prev6'] - train['BAL_prev1'])/(1 + train['I_AQB_PrevQ2'].clip(0, 10**9))\n",
    "train.loc[:,'bal_decline5'] = (train['BAL_prev5'] - train['BAL_prev1'])/(1 + train['I_AQB_PrevQ2'].clip(0, 10**9))\n",
    "train.loc[:,'bal_decline4'] = (train['BAL_prev4'] - train['BAL_prev1'])/(1 + train['I_AQB_PrevQ2'].clip(0, 10**9))\n",
    "train.loc[:,'bal_decline3'] = (train['BAL_prev3'] - train['BAL_prev1'])/(1 + train['I_AQB_PrevQ2'].clip(0, 10**9))\n",
    "train.loc[:,'bal_decline2'] = (train['BAL_prev2'] - train['BAL_prev1'])/(1 + train['I_AQB_PrevQ2'].clip(0, 10**9))\n",
    "\n",
    "test.loc[:,'bal_decline6'] = (test['BAL_prev6'] - test['BAL_prev1'])/(1 + test['I_AQB_PrevQ2'].clip(0, 10**9))\n",
    "test.loc[:,'bal_decline5'] = (test['BAL_prev5'] - test['BAL_prev1'])/(1 + test['I_AQB_PrevQ2'].clip(0, 10**9))\n",
    "test.loc[:,'bal_decline4'] = (test['BAL_prev4'] - test['BAL_prev1'])/(1 + test['I_AQB_PrevQ2'].clip(0, 10**9))\n",
    "test.loc[:,'bal_decline3'] = (test['BAL_prev3'] - test['BAL_prev1'])/(1 + test['I_AQB_PrevQ2'].clip(0, 10**9))\n",
    "test.loc[:,'bal_decline2'] = (test['BAL_prev2'] - test['BAL_prev1'])/(1 + test['I_AQB_PrevQ2'].clip(0, 10**9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mean, std and ratio of debits\n",
    "train.loc[:,'debits_mean'] = train[['D_prev1', 'D_prev2', 'D_prev3', 'D_prev4',\n",
    "                              'D_prev5', 'D_prev6']].apply(lambda r: np.mean(r), axis=1) \n",
    "train.loc[:,'debits_std'] = train[['D_prev1', 'D_prev2', 'D_prev3', 'D_prev4',\n",
    "                              'D_prev5', 'D_prev6']].apply(lambda r: np.std(r), axis=1)\n",
    "train.loc[:,'debit1_rat'] = train['D_prev1']/(1 + train['debits_mean'].clip(0, 10**8))\n",
    "\n",
    "test.loc[:,'debits_mean'] = test[['D_prev1', 'D_prev2', 'D_prev3', 'D_prev4',\n",
    "                              'D_prev5', 'D_prev6']].apply(lambda r: np.mean(r), axis=1) \n",
    "test.loc[:,'debits_std'] = test[['D_prev1', 'D_prev2', 'D_prev3', 'D_prev4',\n",
    "                             'D_prev5', 'D_prev6']].apply(lambda r: np.std(r), axis=1)\n",
    "test.loc[:,'debit1_rat'] = test['D_prev1']/(1 + test['debits_mean'].clip(0, 10**8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Likelihood of EOBprev1 ratio and EOBprev1 after splitting them into categories\n",
    "_, bins1 = pd.qcut(pd.concat([train['EOB_rat1'], test['EOB_rat1']]), \n",
    "                   50, duplicates='drop', retbins=True, labels=False)\n",
    "_, bins2 = pd.qcut(pd.concat([train['EOP_prev1'], test['EOP_prev1']]),\n",
    "                   50, duplicates='drop', retbins=True, labels=False)\n",
    "\n",
    "train.loc[:,'EOB_rat1qcut'] = pd.cut(train['EOB_rat1'], bins=bins1, labels=False)\n",
    "train.loc[:,'EOB_prev1qcut'] = pd.cut(train['EOP_prev1'], bins=bins2, labels=False)\n",
    "\n",
    "test.loc[:,'EOB_rat1qcut'] = pd.cut(test['EOB_rat1'], bins=bins1, labels=False)\n",
    "test.loc[:,'EOB_prev1qcut'] = pd.cut(test['EOP_prev1'], bins=bins2, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "5\n",
      "17\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "#user acc features\n",
    "growth_preclose_feats = ['RD_PREM_CLOSED_PREVQ1' ,'FD_PREM_CLOSED_PREVQ1']\n",
    "\n",
    "#Premature loan closure\n",
    "loan_preclose_feats = [f for f in train.columns if ('_PREM_CLOSED' in f) & (f not in growth_preclose_feats)]\n",
    "\n",
    "#Closure of growth accounts in last quarter\n",
    "growth_close_feats = ['RD_CLOSED_PREVQ1', 'FD_CLOSED_PREVQ1']\n",
    "\n",
    "#CLosure of growth accounts in last year\n",
    "growth_close_feats2 = ['DEMAT_CLOSED_PREV1YR', 'SEC_ACC_CLOSED_PREV1YR']\n",
    "\n",
    "#Closure of loan accounts in last quarter\n",
    "loan_close_feats = train.filter(regex='(?<!PREM)(_Closed_PrevQ1|_CLOSED_PREVQ1)').columns.tolist() + ['CC_CLOSED_PREVQ1']\n",
    "print(len(loan_close_feats))\n",
    "\n",
    "#Live growth accounts\n",
    "growth_accounts_live = ['DEMAT_TAG_LIVE', 'SEC_ACC_TAG_LIVE', 'MF_TAG_LIVE', 'RD_TAG_LIVE', 'FD_TAG_LIVE']\n",
    "print(len(growth_accounts_live))\n",
    "\n",
    "#Live loan accounts\n",
    "loan_accounts_live = train.filter(regex='(_LIVE|live)').columns.tolist()\n",
    "loan_accounts_live = [f for f in loan_accounts_live if f not in growth_accounts_live]\n",
    "print(len(loan_accounts_live))\n",
    "\n",
    "#Loan flags\n",
    "loan_flags = train.filter(regex='_DATE$').columns\n",
    "print(len(loan_flags))\n",
    "\n",
    "train.loc[:,'acc_prem_close'] = train[loan_preclose_feats].fillna(0).apply(lambda row: np.any(row), axis=1)\n",
    "test.loc[:,'acc_prem_close'] = test[loan_preclose_feats].fillna(0).apply(lambda row: np.any(row), axis=1)\n",
    "\n",
    "train.loc[:,'acc_prem_close_sum'] = train[loan_preclose_feats].fillna(0).apply(lambda row: sum(row), axis=1).clip(0,2)\n",
    "test.loc[:,'acc_prem_close_sum'] = test[loan_preclose_feats].fillna(0).apply(lambda row: sum(row), axis=1).clip(0,2)\n",
    "\n",
    "train.loc[:,'loan_close_num'] = train[loan_close_feats].fillna(0).apply(lambda row: sum(row), axis=1).clip(0,3)\n",
    "test.loc[:,'loan_close_num'] = test[loan_close_feats].fillna(0).apply(lambda row: sum(row), axis=1).clip(0,3)\n",
    "\n",
    "train.loc[:,'growth_close_num'] = train[growth_close_feats].fillna(0).apply(lambda row: sum(row), axis=1)\n",
    "test.loc[:,'growth_close_num'] = test[growth_close_feats].fillna(0).apply(lambda row: sum(row), axis=1)\n",
    "\n",
    "train.loc[:,'loan_live'] = train[loan_accounts_live].replace('Y',1).fillna(0).apply(lambda row: sum(row), axis=1).clip(0, 3)\n",
    "test.loc[:,'loan_live'] = test[loan_accounts_live].replace('Y',1).fillna(0).apply(lambda row: sum(row), axis=1).clip(0, 3)\n",
    "\n",
    "train[loan_flags] = train[loan_flags].fillna(0)\n",
    "train.loc[:,'loan_flag_sum1'] = train[loan_flags].apply(lambda row: sum([r==1 for r in row]), \n",
    "                                                  axis=1)\n",
    "train.loc[:,'loan_flag_sum2'] = train[loan_flags].apply(lambda row: sum([r==2 for r in row]), \n",
    "                                                  axis=1)\n",
    "\n",
    "test[loan_flags] = test[loan_flags].fillna(0)\n",
    "test.loc[:,'loan_flag_sum1'] = test[loan_flags].apply(lambda row: sum([r==1 for r in row]), \n",
    "                                                  axis=1)\n",
    "test.loc[:,'loan_flag_sum2'] = test[loan_flags].apply(lambda row: sum([r==2 for r in row]), \n",
    "                                                  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Invest features\n",
    "train.loc[:,'FD_Q1_Q2_diffabs'] = (train['NO_OF_FD_BOOK_PrevQ1'] - train['NO_OF_FD_BOOK_PrevQ2']).clip(-7,7)\n",
    "train.loc[:,'NO_OF_FD_BOOK_PrevQ1'] = train['NO_OF_FD_BOOK_PrevQ1'].clip(0, 25)\n",
    "train.loc[:,'NO_OF_FD_BOOK_PrevQ2'] = train['NO_OF_FD_BOOK_PrevQ2'].clip(0, 25)\n",
    "\n",
    "test.loc[:,'FD_Q1_Q2_diffabs'] = (test['NO_OF_FD_BOOK_PrevQ1'] - test['NO_OF_FD_BOOK_PrevQ2']).clip(-7,7)\n",
    "test.loc[:,'NO_OF_FD_BOOK_PrevQ1'] = test['NO_OF_FD_BOOK_PrevQ1'].clip(0, 25)\n",
    "test.loc[:,'NO_OF_FD_BOOK_PrevQ2'] = test['NO_OF_FD_BOOK_PrevQ2'].clip(0, 25)\n",
    "\n",
    "train.loc[:,'NO_OF_RD_BOOK_PrevQ1'] = train['NO_OF_RD_BOOK_PrevQ1'].clip(0, 20)\n",
    "train.loc[:,'NO_OF_RD_BOOK_PrevQ2'] = train['NO_OF_RD_BOOK_PrevQ2'].clip(0, 20)\n",
    "\n",
    "test.loc[:,'NO_OF_RD_BOOK_PrevQ1'] = test['NO_OF_RD_BOOK_PrevQ1'].clip(0, 20)\n",
    "test.loc[:,'NO_OF_RD_BOOK_PrevQ2'] = test['NO_OF_RD_BOOK_PrevQ2'].clip(0, 20)\n",
    "\n",
    "train.loc[:,'FD_amt_diff'] = train['FD_AMOUNT_BOOK_PrevQ1'] - train['FD_AMOUNT_BOOK_PrevQ2']\n",
    "test.loc[:,'FD_amt_diff'] = test['FD_AMOUNT_BOOK_PrevQ1'] - test['FD_AMOUNT_BOOK_PrevQ2']\n",
    "\n",
    "train.loc[:,'DM_amt_diff'] = train['Dmat_Investing_PrevQ1'] - train['Dmat_Investing_PrevQ2']\n",
    "train.loc[:,'MF_amt_diff'] = train['Total_Invest_in_MF_PrevQ1'] - train['Total_Invest_in_MF_PrevQ2']\n",
    "\n",
    "test.loc[:,'DM_amt_diff'] = test['Dmat_Investing_PrevQ1'] - test['Dmat_Investing_PrevQ2']\n",
    "test.loc[:,'MF_amt_diff'] = test['Total_Invest_in_MF_PrevQ1'] - test['Total_Invest_in_MF_PrevQ2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Credit to Debit ratios\n",
    "\n",
    "train.loc[:, \"C_D_prev1_ratio\"] = train[\"C_prev1\"]/(1+train[\"D_prev1\"])\n",
    "test.loc[:, \"C_D_prev1_ratio\"] = test[\"C_prev1\"]/(1+test[\"D_prev1\"])\n",
    "\n",
    "train.loc[:, \"C_D_prev2_ratio\"] = train[\"C_prev2\"]/(1+ train[\"D_prev2\"])\n",
    "test.loc[:, \"C_D_prev2_ratio\"] = test[\"C_prev2\"]/(1+ test[\"D_prev2\"])\n",
    "\n",
    "train.loc[:, \"C_D_prev3_ratio\"] = train[\"C_prev3\"]/(1+train[\"D_prev3\"])\n",
    "test.loc[:, \"C_D_prev3_ratio\"] = test[\"C_prev3\"]/(1+test[\"D_prev3\"])\n",
    "\n",
    "train.loc[:, \"C_D_prev4_ratio\"] = train[\"C_prev4\"]/(1+train[\"D_prev4\"])\n",
    "test.loc[:, \"C_D_prev4_ratio\"] = test[\"C_prev4\"]/(1+test[\"D_prev4\"])\n",
    "\n",
    "train.loc[:, \"C_D_prev5_ratio\"] = train[\"C_prev5\"]/(1+train[\"D_prev5\"])\n",
    "test.loc[:, \"C_D_prev5_ratio\"] = test[\"C_prev5\"]/(1+test[\"D_prev5\"])\n",
    "\n",
    "train.loc[:, \"C_D_prev6_ratio\"] = train[\"C_prev6\"]/(1+train[\"D_prev6\"])\n",
    "test.loc[:, \"C_D_prev6_ratio\"] = test[\"C_prev6\"]/(1+test[\"D_prev6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_periods = [\"prev1\", \"prev2\", \"prev3\", \"prev4\", \"prev5\", \"prev6\"]\n",
    "useful_channel = [\"CNR_\", \"BAL_\", \"EOP_\"]\n",
    "useful_cr_amb = [\"CR_AMB_Prev1\", \"CR_AMB_Prev2\", \"CR_AMB_Prev3\", \"CR_AMB_Prev4\", \"CR_AMB_Prev5\", \"CR_AMB_Prev6\"]\n",
    "diff_channel_C_columns = [\"count_C_\", \"COUNT_BRANCH_C_\", \"custinit_CR_cnt_\"]\n",
    "diff_channel_D_columns = [\"count_D_\", \"COUNT_ATM_D_\", \"COUNT_BRANCH_D_\", \"COUNT_IB_D_\", \"custinit_DR_cnt_\"]\n",
    "useful_C_channel = [\"BRANCH_C_\", \"custinit_CR_amt_\"]\n",
    "useful_D_channel = [\"ATM_D_\", \"BRANCH_D_\", \"IB_D_\", \"POS_D_\", \"custinit_DR_amt_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_feats_imp = []\n",
    "for d in useful_channel:\n",
    "    cols = []\n",
    "    for t in time_periods:\n",
    "        cols.append(d+t)\n",
    "    a = \"Total_\"+d\n",
    "    train.loc[:, a] = np.average(train[cols], axis = 1, weights=[0.5, 0.25, 0.1, 0.1, 0.05 ,0])\n",
    "    test.loc[:, a] = np.average(test[cols], axis = 1, weights=[0.5, 0.25, 0.1, 0.1, 0.05 ,0])\n",
    "    new_feats_imp.append(a)\n",
    "\n",
    "new_feats_imp.append(\"Total_CR_AMB_\")\n",
    "train.loc[:, \"Total_CR_AMB_\"] = np.average(train[useful_cr_amb], axis = 1, weights=[0.5, 0.25, 0.1, 0.1, 0.05 ,0])\n",
    "test.loc[:, \"Total_CR_AMB_\"] = np.average(test[useful_cr_amb], axis = 1, weights=[0.5, 0.25, 0.1, 0.1, 0.05 ,0])\n",
    "\n",
    "new_feats_cnt = []\n",
    "for d in diff_channel_C_columns:\n",
    "    cols = []\n",
    "    for t in time_periods:\n",
    "        cols.append(d+t)\n",
    "    a = \"Total_\"+d\n",
    "    train.loc[:, a] = np.average(train[cols], axis = 1, weights=[0.3, 0.25, 0.2, 0.15, 0.1 ,0.1])\n",
    "    test.loc[:, a] = np.average(test[cols], axis = 1, weights=[0.3, 0.25, 0.2, 0.15, 0.1 ,0.1])\n",
    "    new_feats_cnt.append(a)\n",
    "for d in diff_channel_D_columns:\n",
    "    cols = []\n",
    "    for t in time_periods:\n",
    "        cols.append(d+t)\n",
    "    a = \"Total_\"+d\n",
    "    train.loc[:, a] = np.average(train[cols], axis = 1, weights=[0.3, 0.25, 0.2, 0.15, 0.1 ,0.1])\n",
    "    test.loc[:, a] = np.average(test[cols], axis = 1, weights=[0.3, 0.25, 0.2, 0.15, 0.1 ,0.1])\n",
    "    new_feats_cnt.append(a)\n",
    "\n",
    "new_feats_amt = []\n",
    "for d in useful_C_channel:\n",
    "    cols = []\n",
    "    for t in time_periods:\n",
    "        cols.append(d+t)\n",
    "    a = \"Total_\"+d\n",
    "    train.loc[:, a] = np.average(train[cols], axis = 1, weights=[0.3, 0.25, 0.2, 0.15, 0.1 ,0.1])\n",
    "    test.loc[:, a] = np.average(test[cols], axis = 1, weights=[0.3, 0.25, 0.2, 0.15, 0.1 ,0.1])\n",
    "    new_feats_amt.append(a)\n",
    "for d in useful_C_channel:\n",
    "    cols = []\n",
    "    for t in time_periods:\n",
    "        cols.append(d+t)\n",
    "    a = \"Total_\"+d\n",
    "    train.loc[:, a] = np.average(train[cols], axis = 1, weights=[0.3, 0.25, 0.2, 0.15, 0.1 ,0.1])\n",
    "    test.loc[:, a] = np.average(test[cols], axis = 1, weights=[0.3, 0.25, 0.2, 0.15, 0.1 ,0.1])\n",
    "    new_feats_amt.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "likeli_feats = [f for f in train.columns if '_likelihood' in f]\n",
    "new_feats = ['CR_mean_drop', 'CR_std_drop', 'EOP_mean_drop', 'EOP_std_drop', 'CR_sum_drop',\n",
    "            'EOB_rat1', 'CR_rat1', 'bal_rats', 'EOP_prev1log', 'brn_bal', 'zip_bal',\n",
    "            'brn_churn', 'zip_churn','bal_decline6', 'bal_decline5', 'bal_decline5',\n",
    "            'bal_decline3','bal_decline2', 'EOB_rat2', 'CR_rat2', 'debits_std', \n",
    "             'debits_mean', 'debit1_rat', 'EOB_rat1qcut', 'EOB_prev1qcut',\n",
    "            'acc_prem_close', 'acc_prem_close_sum', 'loan_close_num', 'growth_close_num',\n",
    "            'loan_live', 'loan_flag_sum1', 'loan_flag_sum2',\n",
    "            'FD_amt_diff', 'DM_amt_diff', 'MF_amt_diff',  'FD_Q1_Q2_diffabs', \"C_D_prev1_ratio\",\n",
    "            \"C_D_prev2_ratio\", \"C_D_prev3_ratio\", \"C_D_prev4_ratio\" ,\"C_D_prev5_ratio\", \"C_D_prev6_ratio\"]\n",
    "all_feats = num_feats + cat_cols + likeli_feats + new_feats + new_feats_imp + new_feats_cnt + new_feats_amt\n",
    "X = train[all_feats]\n",
    "X_test = test[all_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train[\"Responders\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[all_feats + ['Responders','UCIC_ID']].to_csv('../utility/train_wfeats3.csv', index=False, compression='gzip')\n",
    "test[all_feats + ['Responders', 'UCIC_ID']].to_csv('../utility/test_wfeats3.csv', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb.LGBMClassifier.predict_proba_corr = predict_proba_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 20.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 40.1min remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "lgb_params = lgb_params = {\n",
    "    'learning_rate': 0.018,\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 255,\n",
    "    'n_estimators': 500,\n",
    "    #'min_child_weight': 11,\n",
    "    #'min_child_samples': 200,\n",
    "    'subsample':0.95,\n",
    "    'colsample_bytree':0.6,\n",
    "    #'min_sum_hessian_in_leaf':20,\n",
    "    #'reg_lambda': 1,\n",
    "    #'reg_alpha':1,\n",
    "    #'is_unbalance':True,\n",
    "    #'verbose':1,\n",
    "}\n",
    "lgb_preds = cross_val_predict(lgb.LGBMClassifier(**lgb_params), X, y, cv=cvlist, method='predict_proba_corr', verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_top(y, lgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame({'UCIC_ID':train['UCIC_ID'], 'Responders':lgb_preds})\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_preds_test = lgb.LGBMClassifier(**lgb_params).set_params(n_estimators=550).fit(X,y).predict_proba_corr(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(lgb_preds)\n",
    "sns.distplot(lgb_preds_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_test_df = pd.DataFrame({'UCIC_ID':test['UCIC_ID'].values, 'Responders':lgb_preds_test})\n",
    "preds_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_df.to_csv(\"../utility/oof_lgb_feat3_v1.csv\", index=False)\n",
    "preds_test_df.to_csv(\"../utility/test_lgb_feat3_v1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
